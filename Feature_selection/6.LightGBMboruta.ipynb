{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e663d21-dca0-4885-ae70-4eab8cc2f2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "from boruta import BorutaPy\n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb2a4ac-68f9-4d31-a45a-7e17c3ccb746",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ec57b4-25ff-445a-87d4-ef9e53ac0dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_hc= pd.read_csv(\"../Training_residualshealthycontrol_healthycontrol.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0e8f6f-04dc-4a85-bc52-0d57959baa07",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_neuro= pd.read_csv(\"../Training_residualshealthycontrol_neurodegenerative.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627cd975-928b-4f90-9b75-68624aba73e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_hc = training_hc.set_index(\"eid\").drop(columns=training_hc.columns[-1])\n",
    "y_hc = training_hc.set_index(\"eid\")[training_hc.columns[-1]].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9dcf1d-15fa-4386-9d40-1910beacf829",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_neuro = training_neuro.set_index(\"eid\").drop(columns=training_neuro.columns[-1])\n",
    "y_neuro = training_neuro.set_index(\"eid\")[training_neuro.columns[-1]].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bec66fc-e88a-47c2-a019-7746639029d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_neuro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4921b3-7ee1-4797-af51-6af8d3bbefc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_neuro = pd.read_csv(\"Results/Feature_importances_LightGBM_balanced_Training_residualshealthycontrol_neurodegenerative.csv\")\n",
    "\n",
    "results_hc = pd.read_csv(\"Results/Feature_importances_LightGBM_balanced_Training_residualshealthycontrol_healthycontrol.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba74277-935d-4c79-a206-a8f403f1ab81",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_neuro = results_neuro[results_neuro[\"Importance\"] !=0][\"Feature\"].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1631b4c7-2faf-4a80-9bce-99f75434012f",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_hc = results_hc[results_hc[\"Importance\"] !=0][\"Feature\"].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b904bc-b655-4a7e-b379-76facb023b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(features_hc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119ecbcf-5102-4ff8-8f5e-4a675fa58ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf= lgb.LGBMClassifier(random_state=seed, class_weight='balanced', verbose=-1, n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6401b821-b847-44cf-812c-f8ea9e9940f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params = {\n",
    "    'n_estimators': [50, 100,200,500],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'max_depth': [1,5,7,10,15],\n",
    "    'subsample': [0.01, 1],\n",
    "    'colsample_bytree': [0.01, 0.1, 1.0],\n",
    "    'reg_alpha': [0,0.1, 10],\n",
    "    'reg_lambda': [0, 0.1, 10]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c915f0-0cd8-4e4e-be4c-099336aafa32",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(\n",
    "    estimator=clf,\n",
    "    param_grid=lgb_params,\n",
    "    cv=3,\n",
    "    scoring=\"recall\",\n",
    "    n_jobs=4\n",
    ")\n",
    "grid_search.fit(X_hc[features_hc], y_hc)\n",
    "\n",
    "best_model_hc = grid_search.best_estimator_\n",
    "print(\"Best params:\", grid_search.best_params_)\n",
    "print(\"Best CV recall:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f143696e-69f6-46c8-a6a0-4127f5b603d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(X_neuro[features_neuro], y_neuro)\n",
    "\n",
    "best_model_neuro = grid_search.best_estimator_\n",
    "print(\"Best params:\", grid_search.best_params_)\n",
    "print(\"Best CV recall:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee23a406-7335-41b5-be14-59b037eb935b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import check_random_state\n",
    "\n",
    "class BorutaPyForLGB(BorutaPy):\n",
    "    def __init__(self, estimator, n_estimators='auto', min_estimators=50, perc=100, alpha=0.05,\n",
    "                 two_step=True, max_iter=100, random_state=42, verbose=0):\n",
    "        super().__init__(estimator, n_estimators=n_estimators, perc=perc, alpha=alpha,\n",
    "                         two_step=two_step, max_iter=max_iter, random_state=random_state, verbose=verbose)\n",
    "        self._is_lightgbm = 'lightgbm' in str(type(self.estimator))\n",
    "        self.min_estimators = min_estimators\n",
    "\n",
    "    \n",
    "    def _get_tree_num(self, n_features):\n",
    "        \"\"\"Return a safe number of trees for Boruta + LightGBM\"\"\"\n",
    "        if self.n_estimators == 'auto':\n",
    "            n_tree = max(int(n_features * 10), self.min_estimators)\n",
    "            return n_tree\n",
    "        return self.n_estimators\n",
    "        \n",
    "    def _validate_pandas_input(self, arg):\n",
    "        \"\"\"Convert pandas DataFrame/Series to numpy array if needed.\"\"\"\n",
    "        try:\n",
    "            return arg.values\n",
    "        except AttributeError:\n",
    "            raise ValueError(\"Input needs to be a numpy array or pandas DataFrame/Series.\")\n",
    "\n",
    "    \n",
    "        \n",
    "    def _fit(self, X, y):\n",
    "        # check input params\n",
    "        self._check_params(X, y)\n",
    "\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = self._validate_pandas_input(X) \n",
    "        if not isinstance(y, np.ndarray):\n",
    "            y = self._validate_pandas_input(y)\n",
    "\n",
    "        self.random_state = check_random_state(self.random_state)\n",
    "        # setup variables for Boruta\n",
    "        n_sample, n_feat = X.shape\n",
    "        _iter = 1\n",
    "        # holds the decision about each feature:\n",
    "        # 0  - default state = tentative in original code\n",
    "        # 1  - accepted in original code\n",
    "        # -1 - rejected in original code\n",
    "        dec_reg = np.zeros(n_feat, dtype=int)\n",
    "        # counts how many times a given feature was more important than\n",
    "        # the best of the shadow features\n",
    "        hit_reg = np.zeros(n_feat, dtype=int)\n",
    "        # these record the history of the iterations\n",
    "        imp_history = np.zeros(n_feat, dtype=float)\n",
    "        sha_max_history = []\n",
    "\n",
    "        # set n_estimators\n",
    "        if self.n_estimators != 'auto':\n",
    "            self.estimator.set_params(n_estimators=self.n_estimators)\n",
    "\n",
    "        # main feature selection loop\n",
    "        while np.any(dec_reg == 0) and _iter < self.max_iter:\n",
    "            # find optimal number of trees and depth\n",
    "            if self.n_estimators == 'auto':\n",
    "                # number of features that aren't rejected\n",
    "                not_rejected = np.where(dec_reg >= 0)[0].shape[0]\n",
    "                n_tree = self._get_tree_num(not_rejected)\n",
    "                self.estimator.set_params(n_estimators=n_tree)\n",
    "\n",
    "            # make sure we start with a new tree in each iteration\n",
    "            if self._is_lightgbm:\n",
    "                self.estimator.set_params(random_state=self.random_state.randint(0, 10000))\n",
    "            else:\n",
    "                self.estimator.set_params(random_state=self.random_state)\n",
    "\n",
    "            # add shadow attributes, shuffle them and train estimator, get imps\n",
    "            cur_imp = self._add_shadows_get_imps(X, y, dec_reg)\n",
    "\n",
    "            # get the threshold of shadow importances we will use for rejection\n",
    "            imp_sha_max = np.percentile(cur_imp[1], self.perc)\n",
    "\n",
    "            # record importance history\n",
    "            sha_max_history.append(imp_sha_max)\n",
    "            imp_history = np.vstack((imp_history, cur_imp[0]))\n",
    "\n",
    "            # register which feature is more imp than the max of shadows\n",
    "            hit_reg = self._assign_hits(hit_reg, cur_imp, imp_sha_max)\n",
    "\n",
    "            # based on hit_reg we check if a feature is doing better than\n",
    "            # expected by chance\n",
    "            dec_reg = self._do_tests(dec_reg, hit_reg, _iter)\n",
    "\n",
    "            # print out confirmed features\n",
    "            if self.verbose > 0 and _iter < self.max_iter:\n",
    "                self._print_results(dec_reg, _iter, 0)\n",
    "            if _iter < self.max_iter:\n",
    "                _iter += 1\n",
    "\n",
    "        # we automatically apply R package's rough fix for tentative ones\n",
    "        confirmed = np.where(dec_reg == 1)[0]\n",
    "        tentative = np.where(dec_reg == 0)[0]\n",
    "        # ignore the first row of zeros\n",
    "        tentative_median = np.median(imp_history[1:, tentative], axis=0)\n",
    "        # which tentative to keep\n",
    "        tentative_confirmed = np.where(tentative_median\n",
    "                                       > np.median(sha_max_history))[0]\n",
    "        tentative = tentative[tentative_confirmed]\n",
    "\n",
    "        # basic result variables\n",
    "        self.n_features_ = confirmed.shape[0]\n",
    "        self.support_ = np.zeros(n_feat, dtype=bool)\n",
    "        self.support_[confirmed] = 1\n",
    "        self.support_weak_ = np.zeros(n_feat, dtype=bool)\n",
    "        self.support_weak_[tentative] = 1\n",
    "\n",
    "        # ranking, confirmed variables are rank 1\n",
    "        self.ranking_ = np.ones(n_feat, dtype=int)\n",
    "        # tentative variables are rank 2\n",
    "        self.ranking_[tentative] = 2\n",
    "        # selected = confirmed and tentative\n",
    "        selected = np.hstack((confirmed, tentative))\n",
    "        # all rejected features are sorted by importance history\n",
    "        not_selected = np.setdiff1d(np.arange(n_feat), selected)\n",
    "        # large importance values should rank higher = lower ranks -> *(-1)\n",
    "        imp_history_rejected = imp_history[1:, not_selected] * -1\n",
    "\n",
    "        # update rank for not_selected features\n",
    "        if not_selected.shape[0] > 0:\n",
    "                # calculate ranks in each iteration, then median of ranks across feats\n",
    "                iter_ranks = self._nanrankdata(imp_history_rejected, axis=1)\n",
    "                rank_medians = np.nanmedian(iter_ranks, axis=0)\n",
    "                ranks = self._nanrankdata(rank_medians, axis=0)\n",
    "\n",
    "                # set smallest rank to 3 if there are tentative feats\n",
    "                if tentative.shape[0] > 0:\n",
    "                    ranks = ranks - np.min(ranks) + 3\n",
    "                else:\n",
    "                    # and 2 otherwise\n",
    "                    ranks = ranks - np.min(ranks) + 2\n",
    "                self.ranking_[not_selected] = ranks\n",
    "        else:\n",
    "            # all are selected, thus we set feature supports to True\n",
    "            self.support_ = np.ones(n_feat, dtype=bool)\n",
    "\n",
    "        self.importance_history_ = imp_history\n",
    "\n",
    "        # notify user\n",
    "        if self.verbose > 0:\n",
    "            self._print_results(dec_reg, _iter, 1)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29860a53-63bf-4f37-aa02-1e4dc68d4132",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params = {\n",
    "    'n_estimators': [50, 100,200,500],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'max_depth': [1,5,7,10,15],\n",
    "    'subsample': [0.01, 1],\n",
    "    'colsample_bytree': [0.01, 0.1, 1.0],\n",
    "    'reg_alpha': [0,0.1, 10],\n",
    "    'reg_lambda': [0, 0.1, 10]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257497c6-f82c-4d5f-8ec2-7f01120a958a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = lgb.LGBMClassifier(\n",
    "    random_state=seed,\n",
    "    class_weight='balanced',\n",
    "    verbose=-1,\n",
    "    n_jobs=4\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0bbfbb-2628-4b6a-b5c1-d51fd9f3d667",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_neuro[:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b9395e-2c4b-4f96-8c78-b9b91cbddcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_selector = BorutaPyForLGB(best_model_neuro, n_estimators='auto', verbose=0, min_estimators=50,random_state=seed)\n",
    "feat_selector.fit(X_neuro[features_neuro], y_neuro)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e20bb97-b402-41af-b70c-a06b16f60c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the selected features\n",
    "[feat_selector.support_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5466c6c5-e29a-4859-9c5b-c09b6c0bc16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features_neuro_array = np.array(features_neuro)\n",
    "selected_features = features_neuro_array[feat_selector.support_]\n",
    "print(selected_features.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849c2314-82af-4b76-805c-53d272a95ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_selector_hc = BorutaPyForLGB(best_model_hc, n_estimators='auto', verbose=0, min_estimators=50,random_state=seed)\n",
    "feat_selector_hc.fit(X_hc[features_hc], y_hc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e88f5c-f597-46c7-9d08-07ae2fe77aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features_hc_array = np.array(features_hc)\n",
    "selected_features_hc = features_hc_array[feat_selector_hc.support_]\n",
    "print(selected_features_hc.tolist())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_ENVIRONMENT",
   "language": "python",
   "name": "ml_environment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
